#!/usr/bin/env python3
"""
Demo script showing the complete metric sanity reporting pipeline.

This demonstrates the end-to-end flow:
1. QualityAgent generates sanity reports (JSON + Markdown)
2. LangGraph node calls ReportingLLM for explanations
3. MCP tool provides interactive access
"""

import json
from pathlib import Path
from agents.quality_agent import QualityAssuranceAgent
from metric_sanity_explainer import metric_sanity_explainer_node, GraphState
from mcp_tools import handle_metric_sanity_explainer


def demo_pipeline():
    """Run the complete metric sanity pipeline demo."""
    print("ğŸš€ Starting Metric Sanity Reporting Pipeline Demo")
    print("=" * 60)

    # Step 1: Generate sanity reports with QualityAgent
    print("\nğŸ“Š Step 1: Generating metric sanity reports...")
    agent = QualityAssuranceAgent()

    try:
        result = agent.run_metric_sanity_report()
        print("âœ… JSON and Markdown reports generated!")
        print(f"   ğŸ“„ JSON: results/quality/metric_sanity_latest.json")
        print(f"   ğŸ“ MD: results/quality/metric_sanity_latest.md")
        print(f"   ğŸ“Š Status: {result['overall_status']['status']} ({result['overall_status']['severity']})")
        print(f"   ğŸ” Issues: {result['overall_status']['issue_count']}")

    except Exception as e:
        print(f"âŒ Failed to generate reports: {e}")
        return

    # Step 2: LangGraph node processes the report
    print("\nğŸ¤– Step 2: LangGraph node calling ReportingLLM...")

    state = GraphState(
        run_id=result['run_metadata']['run_id'],
        metric_sanity_report_path="results/quality/metric_sanity_latest.json",
        metric_sanity_summary_path="results/quality/metric_sanity_summary.json"
    )

    result_state = metric_sanity_explainer_node(state)
    print("âœ… LLM analysis complete!")
    print(f"   ğŸ“„ Summary: results/quality/metric_sanity_summary.json")
    print(f"   ğŸ“Š Status: {result_state.get('metric_sanity_status')}")
    print(f"   ğŸ” Issues: {result_state.get('metric_sanity_issue_count')}")

    # Step 3: MCP tool for interactive access
    print("\nğŸ”§ Step 3: MCP tool demonstration...")

    # Test structured mode
    structured_result = handle_metric_sanity_explainer({"mode": "structured"})
    print("âœ… Structured mode result:")
    summary = structured_result["content"]
    print(f"   ğŸ“Š Status: {summary.get('status_summary', 'N/A')[:60]}...")
    print(f"   ğŸ” Findings: {len(summary.get('key_findings', []))} items")
    print(f"   ğŸ¯ Actions: {len(summary.get('recommended_actions', []))} items")

    # Test executive mode
    executive_result = handle_metric_sanity_explainer({"mode": "executive"})
    print("âœ… Executive mode result:")
    markdown = executive_result["content"].get("markdown", "")
    print(f"   ğŸ“ Length: {len(markdown)} characters")
    print(f"   ğŸ“‹ Preview: {markdown[:100]}...")

    print("\n" + "=" * 60)
    print("ğŸ‰ Pipeline demo complete!")
    print("\nğŸ“‹ Generated files:")
    print("   â€¢ results/quality/metric_sanity_latest.json")
    print("   â€¢ results/quality/metric_sanity_latest.md")
    print("   â€¢ results/quality/metric_sanity_summary.json")
    print("\nğŸ”„ In production, this would be:")
    print("   â€¢ Called automatically after evaluation")
    print("   â€¢ Feed into Grafana dashboards")
    print("   â€¢ Trigger alerts for critical issues")
    print("   â€¢ Provide LLM explanations for stakeholders")


if __name__ == "__main__":
    demo_pipeline()